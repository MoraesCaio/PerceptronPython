{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import spline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "Perceptron é um classificador binário linear e representa um neurônio, a estrutura básica de uma rede neural. No perceptron, recebe-se os atributos de entrada da base de treinamento (e.g. as entradas de uma porta lógica AND/OR) e multiplica, cada uma delas, por um peso W, conforme Figura 1. Feito isso, os valores resultantes são somados e passam por uma função de ativação.\n",
    "Nesse notebook, todos os passos para implementação do perceptron serão feitos utilizando Numpy, para isso, 5 etapas deverão ser feitas:\n",
    "1. Inicializaçao dos pesos e bias\n",
    "2. Implementando funções de ativação\n",
    "3. Calculando a saída do neurônio\n",
    "4. Predição\n",
    "5. Treino e avaliação\n",
    "\n",
    "![alt text](imgs/perceptron.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1 - Inicialização dos pesos e bias\n",
    "\n",
    "Ao implementar um perceptron, o primeiro passo é iniciar os pesos em um intervalo pequeno, como [-0.5,0.5] aleatoriamente. O bias quando necessário também deve ser inicializado nessa etapa.\n",
    "\n",
    "Para implementar essa etapa, voçê deve utilizar a função weight_init(num_inputs). Dica: você pode utilizar a [função random do numpy](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.random.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'np' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-efa58a21f3bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"what:\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b:\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-efa58a21f3bb>\u001b[0m in \u001b[0;36mweight_init\u001b[0;34m(num_inputs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m### Insira seu código aqui (~2 linhas)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'np' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def weight_init(num_inputs): \n",
    "    \"\"\"\n",
    "    Função que inicializa os pesos e bias aleatoriamente utilizando numpy\n",
    "    Parâmetro: num_inputs - quantidade de entradas X\n",
    "    Retorna: w,b - pesos e bias da rede inicializados\n",
    "    \"\"\"\n",
    "    ### Insira seu código aqui (~2 linhas)\n",
    "    w = np.random.random_sample((num_inputs,)) - 0.5\n",
    "    b = [0.1] * num_inputs\n",
    "    return w,b\n",
    "\n",
    "# test\n",
    "w, b = weight_init(10)\n",
    "print(\"what:\\n\" + str(w))\n",
    "print(\"b:\\n\" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 2 - Implementação das funções de ativação\n",
    "As funções de ativação definem o intervalo de valores que a saída do neurônio poderá ter. Para redes neurais tradicionais, utiliza-se as funções degrau e sigmoid. Redes neurais profundas podem utilizar as funções ReLU, LeakyReLU e Tangente Hiperbólica para evitar problemas no gradiente.\n",
    "\n",
    "Nsse Notebook, as quatro funções de ativação devem ser implementadas, para verificar a corretude das mesmas, a função visualizeActivationFunc exibe os gráficos correspondentes, as funçoes, suas respectivas saídas e gráfico deverão ser similares ao exposto abaixo: (Dica: utilize a [função exp](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html) do numpy)\n",
    "* Degrau: saída 0 se menor que 0 e saída 1 caso contrário\n",
    "$$ \\begin{equation}\n",
    "  degrau =\\begin{cases}\n",
    "    1, & \\text{se $x>0$}.\\\\\n",
    "    0, & \\text{caso contrário}.\n",
    "  \\end{cases}\n",
    "\\end{equation} $$\n",
    "![alt text](imgs/degrau.png \"Title\")\n",
    "* Sigmoid: saída entre [0,1]\n",
    "$$ \\begin{equation}\n",
    "  sigmoid = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation} $$\n",
    "![alt text](imgs/sigmoid.png \"Title\")\n",
    "* Retificadora (Relu): saída 0 caso entrada seja negativa e maior que 1 caso contrário\n",
    "$$ \\begin{equation}\n",
    "  relu = max(0,x)\n",
    "\\end{equation} $$\n",
    "![alt text](imgs/relu.png \"Title\")\n",
    "* Tangente Hiperbólica: saída entre [-1,1]\n",
    "$$ \\begin{equation}\n",
    "  tanh = \\frac{2}{(1+e^{-2*z})} - 1\n",
    "\\end{equation} $$\n",
    "![alt text](imgs/tanh.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIGMOID\n[0.11920292 0.13010847 0.14185106 0.15446527 0.16798161 0.18242552\n 0.19781611 0.21416502 0.23147522 0.24973989 0.26894142 0.2890505\n 0.31002552 0.33181223 0.35434369 0.37754067 0.40131234 0.42555748\n 0.450166   0.47502081 0.5        0.52497919 0.549834   0.57444252\n 0.59868766 0.62245933 0.64565631 0.66818777 0.68997448 0.7109495\n 0.73105858 0.75026011 0.76852478 0.78583498 0.80218389 0.81757448\n 0.83201839 0.84553473 0.85814894 0.86989153 0.88079708]\nSOLO SIGMOID\n0.11920292202211755\nTANH\n[-0.96402758 -0.95623746 -0.94680601 -0.93540907 -0.92166855 -0.90514825\n -0.88535165 -0.86172316 -0.83365461 -0.80049902 -0.76159416 -0.71629787\n -0.66403677 -0.60436778 -0.53704957 -0.46211716 -0.37994896 -0.29131261\n -0.19737532 -0.09966799  0.          0.09966799  0.19737532  0.29131261\n  0.37994896  0.46211716  0.53704957  0.60436778  0.66403677  0.71629787\n  0.76159416  0.80049902  0.83365461  0.86172316  0.88535165  0.90514825\n  0.92166855  0.93540907  0.94680601  0.95623746  0.96402758]\nSOLO TANH\n-0.9640275800758169\nRELU\n[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n 0.  0.  0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.  1.1 1.2 1.3 1.4 1.5\n 1.6 1.7 1.8 1.9 2. ]\nSOLO RELU\n0.0\nDEGRAU\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1]\nSOLO DEGRAU\n1\n"
     ]
    }
   ],
   "source": [
    "def activation_func(func_type, z):\n",
    "    \"\"\"\n",
    "    Funcao que implementa as funcoes de ativacao mais comuns\n",
    "    Parametros: func_type - uma string que contem a funcao de ativacao desejada\n",
    "                z - vetor com os valores de entrada X multiplicado pelos pesos\n",
    "    Retorna: saida da funcao de ativacao\n",
    "    \"\"\"\n",
    "    z = np.asarray(z)\n",
    "    # Seu codigo aqui (~2 linhas)\n",
    "    if func_type == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    elif func_type == 'tanh':\n",
    "        return (2 / (1 + np.exp(-2 * z))) - 1\n",
    "    elif func_type == 'relu':\n",
    "        return np.maximum(0, z)\n",
    "    elif func_type == 'degrau':\n",
    "        return 1 * (z > 0)\n",
    "\n",
    "\n",
    "# test\n",
    "p = [x/10.0 for x in range(-20, 21)]\n",
    "print(\"SIGMOID\\n\" + str(activation_func('sigmoid', p)))\n",
    "print(\"SOLO SIGMOID\\n\" + str(activation_func('sigmoid', p[0])))\n",
    "print(\"TANH\\n\" + str(activation_func('tanh', p)))\n",
    "print(\"SOLO TANH\\n\" + str(activation_func('tanh', p[0])))\n",
    "print(\"RELU\\n\" + str(activation_func('relu', p)))\n",
    "print(\"SOLO RELU\\n\" + str(activation_func('relu', p[0])))\n",
    "print(\"DEGRAU\\n\" + str(activation_func('degrau', p)))\n",
    "print(\"SOLO DEGRAU\\n\" + str(activation_func('degrau', 0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc3 in position 13: ordinal not in range(128)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-11b13744de48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Valores de Saída'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mvisualizeActivationFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-11b13744de48>\u001b[0m in \u001b[0;36mvisualizeActivationFunc\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Entrada'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Valores de Saída'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvisualizeActivationFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mylabel\u001b[0;34m(s, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1575\u001b[0m             \u001b[0mwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m     \"\"\"\n\u001b[0;32m-> 1577\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mset_ylabel\u001b[0;34m(self, ylabel, fontdict, labelpad, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabelpad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabelpad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelpad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_legend_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend_handler_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/axis.pyc\u001b[0m in \u001b[0;36mset_label_text\u001b[0;34m(self, label, fontdict, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \"\"\"\n\u001b[1;32m   1504\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misDefault_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfontdict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/text.pyc\u001b[0m in \u001b[0;36mset_text\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0mACCEPTS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0manything\u001b[0m \u001b[0mprintable\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m'%s'\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \"\"\"\n\u001b[0;32m-> 1212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xc3 in position 13: ordinal not in range(128)"
     ],
     "output_type": "error"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5115644a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.arange(-5., 5., 0.2)\n",
    "def visualizeActivationFunc(z):\n",
    "    z = np.arange(-5., 5., 0.2)\n",
    "    func = []\n",
    "    for i in range(len(z)):\n",
    "        func.append(activation_func('tanh', z[i]))\n",
    "\n",
    "    plt.plot(z,func)\n",
    "    plt.xlabel('Entrada')\n",
    "    plt.ylabel('Valores de Saída')\n",
    "    plt.show()\n",
    "visualizeActivationFunc(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 3 - Cálculo da saída do neurônio¶\n",
    "\n",
    "Com os pesos, bias inicializados e a função de ativação implementada, calcula-se a saída através da equação:\n",
    "\n",
    " $$ \\begin{equation}\n",
    "  Z = W*X + b\n",
    "\\end{equation} $$\n",
    "Feito isso, a saída final é calculada a partir da função de ativação escolhida. Para implementar essa função, você pode utilizar a [função dot do numpy](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) para multiplicar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(w,b,X):\n",
    "    \"\"\"\n",
    "    Função que implementa a etapa forward propagate do neurônio\n",
    "    Parâmetros: w - pesos\n",
    "                b - bias\n",
    "                X - entradas\n",
    "    \"\"\"\n",
    "    ### Seu código aqui (~2 linhas)\n",
    "    z = None\n",
    "    out = None\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 4 - Predição\n",
    "De posse da saída, deve-se avaliar o sucesso da mesma definindo-se um limiar. Para problemas binários, pode-se estabelecer o limiar em 0.5, de forma que abaixo disso a saída é 0 e 1 caso contrário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(out):\n",
    "    \"\"\"\n",
    "    Função que aplica um limiar na saída\n",
    "    Parâmetro: y - saída do neurònio\n",
    "    \"\"\"\n",
    "    ### Seu código aqui (~1 linha)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 5 - Treino e Avaliação\n",
    "\n",
    "Durante o treinamento, a saída é calculada pela função propagate n vezes, onde n é a quantidade de interações do algoritmo. Na primeira interação, os pesos possuem valores pré-definidos pela função de inicialização e são aleatórios após essa interação, as próximas calculam o peso baseado em um erro, calculado a partir da equação:\n",
    "\n",
    " $$ \\begin{equation}\n",
    "  erro = y - ypred\n",
    "\\end{equation} $$\n",
    "\n",
    "Onde y é a saída original do conjunto de dados e y_pred as saidas calculadas. Dado o erro, os pesos são atualizados a partir da equação:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "  w += erro*taxa-de-aprendizado*X\n",
    "\\end{equation} $$\n",
    "\n",
    " \n",
    "Onde X é o conjunto de entrada e a taxa de aprendizagem é um parâmetro de otimização que possui seus valorse variando entre [0,1]. Recomenda-se o uso de taxas de aprendizagem medianas para problemas com redes neurais tradicionais simples (como 0.2-0.5) e taxas de aprendizagem menores para redes neurais profundas (acima de 0.02)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x,y, num_interaction, learning_rate):\n",
    "    \"\"\"\n",
    "    Função que implementa o loop do treinamento \n",
    "    Parâmetros: x - entrada da rede \n",
    "                y - rótulos/labels\n",
    "                num_interaction - quantidade de interações desejada para a rede convergir\n",
    "                learning_rate - taxa de aprendizado para cálculo do erro\n",
    "    \"\"\"\n",
    "    #Passo 1 - Inicie os pesos e bias (~1 linha)\n",
    "    w,b = None\n",
    "    #Passo 2 - Loop por X interações\n",
    "    for j in range(None):\n",
    "        # Passo 3 -  calcule a saída do neurônio (~1 linha)\n",
    "        y_pred = None\n",
    "        # Passo 4 - calcule o erro entre a saída obtida e a saída desejada nos rótulos/labels (~1 linha)\n",
    "        erro = None \n",
    "        # Passo 5 - Atualize o valor dos pesos (~1 linha)\n",
    "        # Dica: você pode utilizar a função np.dot e a função transpose de numpy \n",
    "        w = None \n",
    "        \n",
    "    # Verifique as saídas\n",
    "    print('Saída obtida:', y_pred)\n",
    "    print('Pesos obtidos:', w)\n",
    "\n",
    "    #Métricas de Avaliação\n",
    "    y_pred = predict(y_pred)\n",
    "    print('Matriz de Confusão:')\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    print('F1 Score:')\n",
    "    print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}